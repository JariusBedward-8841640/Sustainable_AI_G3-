{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "",
   "id": "d2961f63e11f62b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# imports",
   "id": "76db6974a71861c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ],
   "id": "4298d1c545a4b68a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenizer & Sop words) Setup for NLP",
   "id": "bece8740ea32f162"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nltk.download(\"stopwords\", quiet=True)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ],
   "id": "f172866605db5901"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load and clean raw data",
   "id": "467b2376a0018ce4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clean_text(text):\n",
    "    #Basic cleaning of data: remove extra spaces and very short string\n",
    "\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text if len(text) > 5 else None\n",
    "\n",
    "def load_clean_raw(csv_path=\"data/raw/raw_prompts.csv\"):\n",
    "    #load csv and clean prompts\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"prompt\"] = df[\"prompt\"].apply(clean_text)\n",
    "    df = df.dropna(subset=[\"prompt\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df_raw = load_clean_raw()\n",
    "print(f\"Raw data loaded:{len(df_raw)} rows\")"
   ],
   "id": "3f0502284cf43713"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Engineering Functions",
   "id": "480e8c262fa29a60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def compute_feature(prompt, num_layers, training_hours, flops_per_hour):\n",
    "    #Compute features for a given prompt and model params\n",
    "\n",
    "\n",
    "\n",
    "    word = prompt.split()\n",
    "    chars = len(prompt)\n",
    "\n",
    "    #token count using tokenizer\n",
    "    token_counter = len(tokenizer.encode(prompt))\n",
    "\n",
    "    #ratio of punctiuatioin characters\n",
    "    punct_ratio = sum(1 for c in prompt if c in \".,!?;:\") /max(chars,1)\n",
    "\n",
    "    #Average word length\n",
    "    avg_word_len = sum(len(w) for w in word) /max(len(word),1)\n",
    "\n",
    "\n",
    "    #Ratio of stopwrods\n",
    "\n",
    "    stopword_ratio = sum(1 for w in word if w.lower() in stop_words) /max(len(word),1)\n",
    "\n",
    "    #Derived numeric features\n",
    "    flops_per_layer = flops_per_hour / max(num_layers,1)\n",
    "    training_efficiency = training_hours / max(num_layers,1)\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"token_count\": token_counter,\n",
    "        \"char_count\": chars,\n",
    "        \"punct_ratio\": punct_ratio,\n",
    "        \"avg_word_length\": avg_word_len,\n",
    "        \"stopword_ratio\": stopword_ratio,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"training_hours\": training_hours,\n",
    "        \"flops_per_hour\": flops_per_hour,\n",
    "        \"flops_per_layer\": flops_per_layer,\n",
    "        \"training_efficiency\": training_efficiency,\n",
    "\n",
    "    }\n",
    "\n",
    "def create_feature_pipeline(df, num_layers_list, training_hours_list, flops_per_hour_list):\n",
    "    rows = []\n",
    "    for i, row in df.itterows():\n",
    "        features = compute_feature(\n",
    "            row[\"prompt\"], num_layers_list, training_hours_list, flops_per_hour_list\n",
    "        )\n",
    "        rows.append(features)\n",
    "    return pd.DataFrame(rows)"
   ],
   "id": "6421c0c70cbaf407"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate Random MOdel params anc ompute features\n",
   "id": "3870a63f87138501"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "n = len(df_raw)\n",
    "layers = np.random.randint(4,48, size=n)\n",
    "hours = np.random.uniform(0.5, 20, size=n)\n",
    "flops = np.random.uniform(1e9, 1e12, size=n)\n",
    "\n",
    "features_df = create_feature_pipeline(df_raw, layers, hours, flops)\n",
    "\n",
    "    #Save processed dataset\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "features_df.to_csv(\"data/processed/features_df.csv\", index=False)\n",
    "print(\"Processed dataset saved to data/processed/features_df.csv\")\n"
   ],
   "id": "77f39c24f4e72da5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate Synth Energy Dataset",
   "id": "95d04ee7608a0308"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "def generate_energy_data(df):\n",
    "    energy = (\n",
    "        df[\"training_hours\"] * 0.5 +\n",
    "        df[\"flops_per_hour\"] * 1e12 +\n",
    "        df[\"num_layers\"] * 0.2 +\n",
    "        df[\"token_count\"] * 0.003 +\n",
    "        df[\"avg_word_len\"] * 0.10\n",
    "    )\n",
    "    #Generate synthetic energy labels for testing\n",
    "\n",
    "    #Create synth energy consumption column for testing purposes\n",
    "    df_energy = df.copy()\n",
    "    df_energy[\"energy_estimate\"] = energy\n",
    "    os.makedirs(\"data/synthetic\", exist_ok=True)\n",
    "    df_energy.to_csv(\"data/synthetic/df_energy.csv\", index=False)\n",
    "    return df_energy\n",
    "\n",
    "energy_df = generate_energy_data(features_df)\n",
    "print(\"Synthetic energy dataset saved to data/synthetic/energy_dataset.csv\")"
   ],
   "id": "88ede2ba3420909c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
