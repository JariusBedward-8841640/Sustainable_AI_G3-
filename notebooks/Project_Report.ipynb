{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Final Project Report: Foundations of Machine Learning Frameworks (CSCN8010)\n",
    "\n",
    "## Project Title:\n",
    "\n",
    "### **Sustainable AI - Transparency and Energy-Efficient Prompt/Context Engineering with Machine Learning**\n",
    "\n",
    "### Team Members:\n",
    "    1. Jarius Bedward - 8841640\n",
    "    2. Mostafa Allahmoradi - 9087818\n",
    "    3. Oluwafemi Lawal - 8967308\n",
    "    4. Jatinder Pal Singh: - 9083762\n",
    "\n",
    "\n",
    "\n",
    "#### Problem Statement:\n",
    "- With the rapid growth of AI systems, there has been a rise in electricity consumption, water, usage, and infrastructure stress in data-center regions around the world. With over a thousand new AI-driven data centers built in 2025 alone, these centers are extremely resource intensive and bring with them significant environment and regulatory challenges. Currently, organizations lack the practical tools for measuring or predicting the energy consumption of prompt design and model usage at the application level. The project aims to bridge the gap by developing a methods to estimate the prompt energy consumption usage and support more transparent, efficient and sustainable AI workflow\n",
    "\n",
    "\n",
    "#### Project Objective:\n",
    "- We set out to design and implement a tool that promotes sustainable AI usage by estimating and reducing the energy cost of user prompts. To achieve this we developed a streamlit based chatbot application built in Python that analyzes a prompt, evaluates it's structure and characteristics such as word count, character count, token count and predict its' estimated energy consumption using machine learning models trained on the dataset.\n",
    "\n",
    "- The application supports sustainable prompt practices but generating and recommending an optimized version of the user's prompt that preserves the original meaning while reducing complexity which would result in lower energy cost.\n"
   ],
   "id": "76efff44ecfac9f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Team Roles and Work Distribution:\n",
    "- We divided the work into 4 equal different parts to be able to work on it together\n",
    "| Member      | Role                                             |\n",
    "|------------|--------------------------------------------------|\n",
    "|  Jarius    |  Project Setup, Data Pre-Processing & Feature Engineering\n",
    "|  Mostafa   |  Energy Prediction ML Model + GUI Integration (Streamlit)\n",
    "|  Oluwafemi |  NLP/ Prompt Analysis\n",
    "|  Jatinder  |  Energy Prediction ML Model + GUI integration (Streamlit)\n"
   ],
   "id": "1599c8a535c7f759"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Methodology:\n",
    "\n",
    "4. To accomplish our goal, our methodology involved dataset construction, feature engineering, model training and interface development to create a system capable of estimating and visualizing prompt level energy usage. We needed to be able to have prompts that also have energy readings to be able to train the model so it could understand the relationship between prompt complexity and energy usage. We then needed to be able to visualize the data to interpret how well our models are performing and what we can do to better tune them.\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "9593d4cf44c370d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "4.1. Data Source and Synthetic Energy Labels\n",
    "\n",
    "- We imported a hugging face dataset that contains text from yelp reviews, which was the base for generating optimized version of prompts as well as understanding how much energy consumption a prompt would take. While the prompts were real, the energy metrics as well as other metrics were not available. To fix this, we designed a formula to produced synthetic but meaningful targets. The generated labels allow us to create a controlled environment for testing and how text complexity could influence predicted resource usage.\n",
    "\n",
    "- Just because the energy readings are synthetic does not make the project void as well. The program is designed to be able to just simply swap out datasets and if the energy reading is provided within the dataset it would be able to read it and the models would be trained based on that new data with accurate energy readings.\n",
    "\n"
   ],
   "id": "35705c0e15e1525c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Load dataset\n",
    "\n",
    "def load_dataset_raw():\n",
    "    os.makedirs(\"../data/raw\", exist_ok=True)\n",
    "    #Load yelp review (full train split)\n",
    "    dataset = load_dataset('yelp_review_full' ,split=\"train[:100000]\") #Limit to 100,000 instead of full 650,000 for much better performance and stability\n",
    "\n",
    "    print(f\"Loaded full dataset with {len(dataset)} rows.\")\n",
    "    #create a df with only prompt text\n",
    "    df = pd.DataFrame({\"prompt\": [x[\"text\"] for x in dataset]})\n",
    "    df = df.rename(columns={\"text\": \"prompt\"})\n",
    "\n",
    "    #save raw data to CSV\n",
    "    df.to_csv(\"data/raw/raw_prompts.csv\", index=False)\n",
    "    return df"
   ],
   "id": "f8820464ddc20212"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Snippet of energy usage calculation\n",
    "\n",
    "#Creating synthetic data based on raw dataset\n",
    "def generate_energy_data(df):\n",
    "    #Generate synthetic energy labels for testing\n",
    "    df = df.copy()\n",
    "    df[\"energy_kwh\"] = (\n",
    "        0.5 #baseline energy cost\n",
    "        + df[\"token_count\"] * 0.003 #increass energy proportionally with number of tokens in prompt\n",
    "        + df[\"avg_word_length\"] * 0.10 # slightly increases energy prompt with longer words\n",
    "        + df [\"token_count\"] * df[\"avg_word_length\"] * 0.001\n",
    "        + (df[\"token_count\"] ** 2) * 0.00005\n",
    "        + np.random.normal(0,0.2, size=len(df)) #small noise to set to make dataset more realistic and avoid perfect linearity\n",
    "    )\n",
    "    df.to_csv(\"data/synthetic/energy_dataset.csv\", index=False)\n",
    "    return df"
   ],
   "id": "7d746a3a3e97c379"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "4.2  Feature Engineering\n",
    "- Each prompt was processed to extract linguistic and structural characteristics like word count, token count and average word length. These features were chosen because they all reflect prompt complexity\n",
    "\n",
    "- By quantifying these aspects we provide the model with signals that could help it learn the relationship between text structure and the energy metrics"
   ],
   "id": "da35b4c6f989377b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Snippet of the feature engineering\n",
    "from src.utils.feature_engineering import tokenizer, stop_words\n",
    "\n",
    "def compute_feature(prompt, num_layers, training_hours, flops_per_hour):\n",
    "    #Compute features for a given prompt and model params\n",
    "    word = prompt.split()\n",
    "    chars = len(prompt)\n",
    "    #token count using tokenizer #truncate to avoid maximum sequence index error so that all 650,000 rows are processed\n",
    "    token_counter = len(tokenizer.encode(prompt, truncation=True, max_length=512))\n",
    "    #ratio of punctiuatioin characters\n",
    "    punct_ratio = sum(1 for c in prompt if c in \".,!?;:\") /max(chars,1)\n",
    "    #Average word length\n",
    "    avg_word_len = sum(len(w) for w in word) /max(len(word),1)\n",
    "    stopword_ratio = sum(1 for w in word if w.lower() in stop_words) /max(len(word),1)\n",
    "    #Derived numeric features\n",
    "    flops_per_layer = flops_per_hour / max(num_layers,1)\n",
    "    training_efficiency = training_hours / max(num_layers,1)"
   ],
   "id": "1c382779310a22ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "4.3 Model Selection & Training\n",
    "- We chose two suprivced learning approaches\n",
    "    - Linear Regression:\n",
    "        - The model provides a simple interpretable baseline that helps reveal whether the relationship between text features and synthetic labels ir roughly linear. It's useful for understanding feature influence even if it is more complex\n",
    "    - Random Forrest\n",
    "        - This was selected because it handles the relationships and feature interactions effectively. Since prompt complexity can change energy usage in even little ways, Random Forest provides flexibility and typically would give higher accuracy predictions which made it a good choice for the project\n",
    "- Both projects were trained on the same dataset and we included both models to allow us to compare performance and choose the most suitable predictor for the application"
   ],
   "id": "1dc13e70ae5553ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    " #Snippet of model selection\n",
    " if model_type == \"RandomForest\":\n",
    "            self.model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        elif model_type == \"LinearRegression\":\n",
    "            self.model = LinearRegression()\n",
    "        else:\n",
    "            raise ValueError(\"model_type must be 'RandomForest' or 'LinearRegression'\")\n"
   ],
   "id": "a558737bdac00154"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "4.4 Visualization & Interpretation\n",
    "- To support our work, we designed plots that would compare the model's predicted values vs the actual energy values. These plots can be seen on the main page of the application after entering the prompt\n",
    "- These visualizations can help both users and developers understand the model accuracy, identify biases and asses what additional features or adjustments are needed"
   ],
   "id": "421d4a4451358f4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Snippet of plot for actual and predicted  for eneergy consumption\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        ax.plot(x, y_real, label=\"Actual\", color=\"#1f77b4\", marker=\"o\")\n",
    "        ax.plot(x, preds_real, label=\"Predicted\", color=\"#ff7f0e\",\n",
    "                linestyle=\"--\", marker=\"x\")\n"
   ],
   "id": "fc4ff10d39375d53"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "4.5 Prompt Optimization\n",
    "\n",
    "- To encourage more efficient prompt engineering, the system also includes an optimization feature. When activated, it generates a rewritten version of the user's prompt that retains the original meaning but reduces complexity. This prompts awareness of how phrasing impacts energy usafe and helps users learn how to create cleaner, concise prompts\n"
   ],
   "id": "780a9c46a9edc32b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    " def optimize(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        max_length: int = 128,\n",
    "        num_beams: int = 4,\n",
    "        temperature: float = 0.7,\n",
    "        do_sample: bool = False\n",
    "    ) -> Tuple[str, List[str]]:\n",
    "\n",
    "        if not prompt or not prompt.strip():\n",
    "            return prompt, []\n",
    "\n",
    "        changes_made = []\n",
    "\n",
    "\n",
    "        rule_optimized, rule_changes = self._fallback_optimize(prompt)\n",
    "        changes_made.extend(rule_changes)\n",
    "\n",
    "        orig_tokens = self._count_tokens(prompt)\n",
    "        rule_tokens = self._count_tokens(rule_optimized)\n",
    "\n",
    "        # If rule-based already achieved significant reduction, use it\n",
    "        rule_reduction = (orig_tokens - rule_tokens) / max(orig_tokens, 1)"
   ],
   "id": "2617afe4e5de7e38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### System Architecture:\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "c6ac642bc357b86d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "5 . Data Input and Pre Processing Layer\n",
    "- The system loads the text from dataset. The text is passed to the pre processing module which cleans the data by normalizing and performs tokenization, counts words and extraction of text complexity metrics"
   ],
   "id": "3771d39b95ece8f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "  #Load data snippet\n",
    "   dataset = load_dataset('yelp_review_full' ,split=\"train[:100000]\")\n",
    "\n",
    "    print(f\"Loaded full dataset with {len(dataset)} rows.\")\n",
    "\n",
    "    df = pd.DataFrame({\"prompt\": [x[\"text\"] for x in dataset]})\n",
    "    df = df.rename(columns={\"text\": \"prompt\"})\n",
    "\n",
    "\n",
    "\n",
    "  #Tokken Snippet\n",
    "  tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n"
   ],
   "id": "fdaeaa58c755b42"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "5.1 Feature Engineering\n",
    "- The extracted test metrics (word count, token count, average word length) are compiled and added as features to the dataset."
   ],
   "id": "d7243a17a1dbc66a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_feature(prompt, num_layers, training_hours, flops_per_hour):\n",
    "    #Compute features for a given prompt and model params\n",
    "\n",
    "    word = prompt.split()\n",
    "    chars = len(prompt)\n",
    "\n",
    "    token_counter = len(tokenizer.encode(prompt, truncation=True, max_length=512))\n",
    "\n",
    "    #ratio of punctiuatioin characters\n",
    "    punct_ratio = sum(1 for c in prompt if c in \".,!?;:\") /max(chars,1)\n",
    "\n",
    "    #Average word length\n",
    "    avg_word_len = sum(len(w) for w in word) /max(len(word),1)\n",
    "\n",
    "\n",
    "    #Ratio of stopwrods\n",
    "\n",
    "    stopword_ratio = sum(1 for w in word if w.lower() in stop_words) /max(len(word),1)\n",
    "\n",
    "    #Derived numeric features\n",
    "    flops_per_layer = flops_per_hour / max(num_layers,1)\n",
    "    training_efficiency = training_hours / max(num_layers,1)\n"
   ],
   "id": "836e686f11abdad4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "5.2 Synthetic Energy Label\n",
    "- A custom formula assigns synthetic values for energy consumption. This labels long with FLOPs per hour and training efficiency form the target values for model training and a synthetic dataset is saved to be used for training"
   ],
   "id": "7e0ccb3a5838057d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    " df = df.copy()\n",
    "    df[\"energy_kwh\"] = (\n",
    "        0.5 #baseline energy cost\n",
    "        + df[\"token_count\"] * 0.003 #increass energy proportionally with number of tokens in prompt\n",
    "        + df[\"avg_word_length\"] * 0.10 # slightly increases energy prompt with longer words\n",
    "        + df [\"token_count\"] * df[\"avg_word_length\"] * 0.001\n",
    "        + (df[\"token_count\"] ** 2) * 0.00005\n",
    "        + np.random.normal(0,0.2, size=len(df)) #small noise to set to make dataset more realistic and avoid perfect linearity\n",
    "    )\n",
    "    df.to_csv(\"data/synthetic/energy_dataset.csv\", index=False)"
   ],
   "id": "677f3bcd1144def2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "5.3 Machine Learning Model\n",
    "- Two models are implemented to take the engineered features & text data and are trained to learn the relationship between text prompt complexity and energy usage"
   ],
   "id": "b6b4302a7e301be1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T23:03:49.407823Z",
     "start_time": "2025-12-11T23:03:49.402551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Model selection Snippet\n",
    "     if model_type == \"RandomForest\":\n",
    "            self.model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        elif model_type == \"LinearRegression\":\n",
    "            self.model = LinearRegression()\n",
    "        else:\n",
    "\n",
    "\n",
    "# Train model snippet\n",
    "\n",
    " def train(self):\n",
    "        print(f\"--- Starting Training ({self.model_type}) ---\")\n",
    "\n",
    "        (X_train, y_train), (X_test, y_test) = self.preprocess_and_split()\n",
    "\n",
    "        # Flatten y_train for sklearn\n",
    "        # ravel is used to convert a multi-dimensional array into a one-dimensional (1D) array.\n",
    "        self.model.fit(X_train, y_train.ravel())\n",
    "        self.anomaly_model.fit(X_train)\n",
    "\n",
    "        self.is_trained = True\n",
    "        self._evaluate(X_test, y_test)\n",
    "\n",
    "        print(f\"Training complete. R2 Score: {self.metrics['r2']:.4f}\")\n"
   ],
   "id": "7b87909caddb9288",
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001B[36mFile \u001B[39m\u001B[32m<string>:3\u001B[39m\n\u001B[31m    \u001B[39m\u001B[31melif model_type == \"LinearRegression\":\u001B[39m\n                                          ^\n\u001B[31mIndentationError\u001B[39m\u001B[31m:\u001B[39m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "5.4 Prediction & Visualization\n",
    "- When a user enters a prompt the system processes the text, extracts the features, runs the trained model to predict the energy usage and then generates plots comparing the predicted and actual values. The metrics are all displayed with summaries and plots on the UI"
   ],
   "id": "b720da896d17e3d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Prediction Snippet\n",
    "    def estimate(self, prompt_text, layers, training_hours, flops_str):\n",
    "        # 1. Parse Inputs\n",
    "        token_count = ComplexityAnalyzer.get_token_count(prompt_text)\n",
    "\n",
    "        try:\n",
    "            flops_val = float(flops_str)\n",
    "            flops_score = (flops_val / 1e18) * 100\n",
    "            if flops_score < 10: flops_score = 10\n",
    "        except ValueError:\n",
    "            flops_score = 10.0\n",
    "\n",
    "        input_data = pd.DataFrame({\n",
    "            'num_layers': [layers],\n",
    "            'training_hours': [training_hours],\n",
    "            'flops_per_hour': [flops_score],\n",
    "            'token_count': [token_count],\n",
    "            'avg_word_length': [ComplexityAnalyzer.get_avg_word_length(prompt_text)]\n",
    "        })\n",
    "\n",
    "        # Get raw prediction\n",
    "        param = input_data.iloc[0].to_dict()\n",
    "        predicted_energy = self.predict_energy(param)\n",
    "        predicted_energy = max(0.1, predicted_energy)\n",
    "\n",
    "        suggestion = \"âœ… Optimized.\"\n",
    "        if predicted_energy > 50:\n",
    "            suggestion = \"âš ï¸ High Consumption. Consider reducing layers.\"\n",
    "        elif token_count > 1000:\n",
    "            suggestion = \"âš ï¸ Context too long. Summarize input.\"\n",
    "\n",
    "        return {\n",
    "            \"energy_kwh\": round(predicted_energy, 4),\n",
    "            \"carbon_kg\": round(predicted_energy * 0.475, 4),\n",
    "            \"suggestion\": suggestion,\n",
    "            \"token_count\": token_count,\n",
    "            \"predicted_val\": predicted_energy\n",
    "        }\n",
    "\n",
    "\n",
    "#Plot pred vs actual snippet\n",
    "  # Axis settings\n",
    "    ax.set_title(f\"Energy Prediction vs Actual ({self.model_type})\")\n",
    "    ax.set_xlabel(\"Number of Layers\")\n",
    "    ax.set_ylabel(\"Energy Consumption (kWh)\")\n",
    "    ax.grid(True, alpha=0.4)\n",
    "    ax.legend()"
   ],
   "id": "11175530e82466f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "5.5 Training Optimization\n",
    "- The optimizer is trained based on different metrics like semantics score or quality and is then evaluated on its performance"
   ],
   "id": "4bd92b6ebf076e29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate model snippet\n",
    "def evaluate_model(optimizer: T5PromptOptimizer, test_prompts: list) -> dict:\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Evaluating Model Performance\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    similarity_checker = SemanticSimilarity()\n",
    "    validator = EnhancedPromptValidator()\n",
    "\n",
    "    results = {\n",
    "        \"total_prompts\": len(test_prompts),\n",
    "        \"avg_token_reduction\": 0,\n",
    "        \"avg_semantic_similarity\": 0,\n",
    "        \"avg_quality_score\": 0,\n",
    "        \"meaning_preserved_count\": 0,\n",
    "        \"by_category\": {}\n",
    "    }\n",
    "\n",
    "    total_token_reduction = 0\n",
    "    total_similarity = 0\n",
    "    total_quality = 0\n",
    "\n",
    "    for i, test in enumerate(test_prompts):\n",
    "        original = test.get('original', '')\n",
    "        expected = test.get('optimized', '')\n",
    "        category = test.get('category', 'general')\n",
    "\n",
    "        # Get optimization\n",
    "        opt_result = optimizer.get_full_optimization(original)\n",
    "\n",
    "        # Validate\n",
    "        validation = validator.validate_prompt_optimization(original, opt_result.optimized_prompt)\n",
    "\n",
    "        # Track metrics\n",
    "        total_token_reduction += opt_result.token_reduction\n",
    "        total_similarity += validation['semantic_similarity']\n",
    "        total_quality += validation['quality_score']\n",
    "\n",
    "        if validation['is_valid']:\n",
    "            results['meaning_preserved_count'] += 1\n",
    "\n",
    "        # Track by category\n",
    "        if category not in results['by_category']:\n",
    "            results['by_category'][category] = {\n",
    "                'count': 0,\n",
    "                'avg_reduction': 0,\n",
    "                'avg_similarity': 0\n",
    "            }\n",
    "        results['by_category'][category]['count'] += 1\n",
    "\n",
    "        # Print sample results (first 5)\n",
    "        if i < 5:\n",
    "            print(f\"\\nTest {i+1}:\")\n",
    "            print(f\"  Original: {original[:80]}...\")\n",
    "            print(f\"  Optimized: {opt_result.optimized_prompt}\")\n",
    "            print(f\"  Expected: {expected}\")\n",
    "            print(f\"  Token Reduction: {opt_result.token_reduction}%\")\n",
    "            print(f\"  Similarity: {validation['semantic_similarity']:.2%}\")\n",
    "\n",
    "    # Calculate averages\n",
    "    n = len(test_prompts)\n",
    "    results['avg_token_reduction'] = round(total_token_reduction / n, 1)\n",
    "    results['avg_semantic_similarity'] = round(total_similarity / n, 3)\n",
    "    results['avg_quality_score'] = round(total_quality / n, 3)\n",
    "    results['meaning_preservation_rate'] = round(results['meaning_preserved_count'] / n * 100, 1)\n",
    "\n",
    "    return results"
   ],
   "id": "e5435e3ac7b690e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "5.6 Prompt Optimizer\n",
    "- The T5 model is combined with rule-based transformation to rephrase prompts  to be more energy-efficient while minting semantic meaning. If the user selects the optimization option, the system rewrites the prompt and the optimized prompt is re-evaluated and presented with updated predictions in an attempt to lower energy consumption\n"
   ],
   "id": "144cfcd2b1c052fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    " #Optimizer Snipet\n",
    " def get_full_optimization(self, prompt: str) -> OptimizationResult:\n",
    "        \"\"\"\n",
    "        Get complete optimization result with all metrics.\n",
    "\n",
    "        Args:\n",
    "            prompt: Original prompt to optimize\n",
    "\n",
    "        Returns:\n",
    "            OptimizationResult with all optimization metrics\n",
    "        \"\"\"\n",
    "        original_tokens = self._count_tokens(prompt)\n",
    "\n",
    "        # Optimize (now returns tuple)\n",
    "        optimized, changes_made = self.optimize(prompt)\n",
    "        optimized_tokens = self._count_tokens(optimized)\n",
    "\n",
    "        # Calculate metrics\n",
    "        token_reduction = 1 - (optimized_tokens / max(original_tokens, 1))\n",
    "        token_reduction = max(0, token_reduction)  # Ensure non-negative\n",
    "\n",
    "        # Estimate energy reduction (proportional to token reduction)\n",
    "        # Based on research: energy ~ O(n^2) for transformer attention\n",
    "        energy_reduction = 1 - ((optimized_tokens / max(original_tokens, 1)) ** 2)\n",
    "        energy_reduction = max(0, min(1, energy_reduction))\n",
    "\n",
    "        # Semantic similarity\n",
    "        semantic_similarity = self._estimate_semantic_similarity(prompt, optimized)\n",
    "\n",
    "        # Quality rating based on both reduction AND semantic preservation\n",
    "        if token_reduction > 0.4 and semantic_similarity > 0.7:\n",
    "            quality = \"Excellent\"\n",
    "        elif token_reduction > 0.25 and semantic_similarity > 0.6:\n",
    "            quality = \"Good\"\n",
    "        elif token_reduction > 0.1 and semantic_similarity > 0.5:\n",
    "            quality = \"Moderate\"\n",
    "        elif token_reduction > 0:\n",
    "            quality = \"Minimal\"\n",
    "        else:\n",
    "            quality = \"No Change\"\n",
    "\n",
    "        return OptimizationResult(\n",
    "            original_prompt=prompt,\n",
    "            optimized_prompt=optimized,\n",
    "            original_tokens=original_tokens,\n",
    "            optimized_tokens=optimized_tokens,\n",
    "            token_reduction=round(token_reduction * 100, 1),\n",
    "            semantic_similarity=round(semantic_similarity * 100, 1),\n",
    "            energy_reduction_estimate=round(energy_reduction * 100, 1),\n",
    "            optimization_quality=quality,\n",
    "            changes_made=changes_made\n",
    "        )\n",
    "\n",
    "#prompt optimize snippet\n",
    "def optimize_prompt(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Quick function to optimize a single prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt: The prompt to optimize\n",
    "\n",
    "    Returns:\n",
    "        Optimized prompt string\n",
    "    \"\"\"\n",
    "    optimizer = T5PromptOptimizer()\n",
    "    optimized, _ = optimizer.optimize(prompt)\n",
    "    return optimized"
   ],
   "id": "16c865bc5b568830"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "5.7 User Interface (Streamlit)\n",
    "- Streamlit serves as the front-end environment where\n",
    "    - The user enters the prompt\n",
    "    - THe predictions and visualizations are displayed\n",
    "    - Other metrics can be viewed\n",
    "    - Optimization can be applied\n",
    "    - All visualizations are updated interactively"
   ],
   "id": "ed3768d04cb02b12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Streamlit GUI Snippet\n",
    "\n",
    "# -------------------------------------\n",
    "# PROMPT INPUT VIEW\n",
    "# -------------------------------------\n",
    "def input_prompt_view():\n",
    "    st.subheader(\"Enter Prompt Context\")\n",
    "    return st.text_area(\"Input Prompt:\", height=100, placeholder=\"Enter your prompt here...\")\n",
    "\n",
    "# -------------------------------------\n",
    "# INITIAL ANALYSIS METRICS\n",
    "# -------------------------------------\n",
    "def render_initial_analysis(original, optimization):\n",
    "    st.divider()\n",
    "    st.subheader(\"ðŸ“Š Analysis Report\")\n",
    "\n",
    "    col1, col2, col3, col4 = st.columns(4)\n",
    "    col1.metric(\"Predicted Energy\", f\"{original['energy_kwh']} kWh\")\n",
    "    col2.metric(\"Carbon Footprint\", f\"{original['carbon_kg']} kgCO2\")\n",
    "    col3.metric(\"Original Tokens\", optimization.get('original_tokens', original['token_count']))\n",
    "    col4.metric(\"Optimized Tokens\", optimization.get('optimized_tokens', '-'))\n",
    "\n",
    "    st.info(\"**Message: âœ… Initial analysis complete.**\")\n",
    "\n",
    "# -------------------------------------\n",
    "# TRAINING PERFORMANCE PLOT\n",
    "# -------------------------------------\n",
    "def render_training_plot(estimator, layers):\n",
    "    st.markdown(\"#### Initial Model Performance\")\n",
    "    with st.expander(\"ðŸ“ˆ View Energy Performance Graph\", expanded=True):\n",
    "        fig = estimator.get_training_plot(layers)\n",
    "        st.pyplot(fig)"
   ],
   "id": "e69cf00bd9a41e99"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Challenges:\n",
    "- Some challenges we faced along the way include:\n",
    "\n",
    "    - Finding the right amount of data to use. (Too much and the model took too long to train and predict, Too little and the model couldn't predict well or optimize properly).\n",
    "\n",
    "    - Applying propper optimization to prompts. (Sometimes the output of a prompt wouldn't resemble the original prompt at all).\n",
    "\n",
    "    - Being able to properly visualize the prediction label on streamlit gui. (There were issues with being able to plot the labels that were needed for the plots while using streamlit).\n"
   ],
   "id": "59a5fbbeb1406a14"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conclusion:\n",
    "- In conclusion, we were able to effectively collaborate and create a working program that takes the users prompts outputs a multitude of metrics including energy consumption, plots it on a graph to see the predicted vs actual, and then allow the user to optimize their prompt to help promote sustainability in AI.\n",
    "\n",
    "- The next step that could be taken to continue to bring the program to the next level is being able to apply the optimizer to the chatbot itself to process a prompt, optimize it and then give it to the chat bot to then consume less energy then it would have before"
   ],
   "id": "858d393d73cc687a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
